{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-05T08:34:44.500369Z",
     "start_time": "2024-03-05T08:34:44.295828Z"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load XML file\n",
    "tree = ET.parse('../data/WikToR.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# List to store dictionaries\n",
    "pages_list = []\n",
    "\n",
    "# Iterate over each <page> element\n",
    "for page in root.findall('.//page'):\n",
    "    page_dict = {}\n",
    "\n",
    "    # Extract data from each <page> element\n",
    "    page_dict['number'] = page.get('number')\n",
    "    page_dict['pageTitle'] = page.find('pageTitle').text\n",
    "    page_dict['toponymName'] = page.find('toponymName').text\n",
    "    page_dict['text'] = page.find('text').text\n",
    "    page_dict['url'] = page.find('url').text\n",
    "    page_dict['lat'] = page.find('lat').text\n",
    "    page_dict['lon'] = page.find('lon').text\n",
    "    page_dict['feature'] = page.find('feature').text\n",
    "    page_dict['country'] = page.find('country').text\n",
    "\n",
    "    # Extract data from <toponymIndices>\n",
    "    toponym_indices = []\n",
    "    for toponym in page.findall('.//toponymIndices/toponym'):\n",
    "        index_dict = {\n",
    "            'start': int(toponym.find('start').text),\n",
    "            'end': int(toponym.find('end').text)\n",
    "        }\n",
    "        toponym_indices.append(index_dict)\n",
    "    page_dict['toponymIndices'] = toponym_indices\n",
    "\n",
    "    # Append page dictionary to the list\n",
    "    pages_list.append(page_dict)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T08:34:45.445798Z",
     "start_time": "2024-03-05T08:34:45.439359Z"
    }
   },
   "id": "2c4468e5d16d7fed",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/miniconda3/envs/geonlp/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/thomas/miniconda3/envs/geonlp/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from genre.trie import Trie\n",
    "\n",
    "# load the prefix tree (trie)\n",
    "with open(\"../data/kilt_titles_trie_dict.pkl\", \"rb\") as f:\n",
    "    trie = Trie.load_from_dict(pickle.load(f))\n",
    "\n",
    "from genre.hf_model import GENRE\n",
    "\n",
    "model = GENRE.from_pretrained(\"../models/hf_entity_disambiguation_aidayago\").eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T08:35:31.477145Z",
     "start_time": "2024-03-05T08:34:53.479168Z"
    }
   },
   "id": "c4a89df2c6eb82b8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_coordinates(place_name):\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": place_name,\n",
    "        \"prop\": \"coordinates\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        PAGES = DATA['query']['pages']\n",
    "\n",
    "        for k, v in PAGES.items():\n",
    "            coordinates = v.get('coordinates')\n",
    "            if coordinates:\n",
    "                lat = round(float(coordinates[0]['lat']), 4)\n",
    "                lon = round(float(coordinates[0]['lon']), 4)\n",
    "                return lat, lon\n",
    "        return 0, 0\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return 0, 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T09:03:34.099443Z",
     "start_time": "2024-03-05T09:03:34.095915Z"
    }
   },
   "id": "4665ffbbfab0ecaf",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the Earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    radius_of_earth = 6371  # Radius of the Earth in kilometers\n",
    "    distance = radius_of_earth * c\n",
    "\n",
    "    return distance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T09:03:34.341096Z",
     "start_time": "2024-03-05T09:03:34.336892Z"
    }
   },
   "id": "755a7d45d7ac6666",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victoria, Seychelles\n",
      "\tIncorrect. Got: Victoria, Chile\n",
      "\t12838.35504879085\n",
      "\tIncorrect. Got: Port Victoria\n",
      "\t6180.510501800203\n",
      "\tIncorrect. Got: Port Victoria\n",
      "\t6180.510501800203\n",
      "\tCorrect\n",
      "\t0.21779862287022636\n",
      "Victoria, Gozo\n",
      "\tCorrect\n",
      "\t4269.793132696058\n",
      "\tCorrect\n",
      "\t4269.793132696058\n",
      "\tCorrect\n",
      "\t4269.793132696058\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the modified list of dictionaries\n",
    "for page_dict in pages_list[11:13]:\n",
    "    print(page_dict['pageTitle'])\n",
    "    text = page_dict['text']\n",
    "    for index_pair in page_dict['toponymIndices']:\n",
    "        start_index = index_pair['start']\n",
    "        end_index = index_pair['end']\n",
    "        modified_text = text[:start_index] + \" [START_ENT] \" + text[start_index:end_index] + \" [END_ENT] \" + text[\n",
    "                                                                                                             end_index:]\n",
    "\n",
    "        sentences = [modified_text]\n",
    "        res = model.sample(\n",
    "            sentences,\n",
    "            prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n",
    "        )\n",
    "        # pprint(res)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        if res[0][0][\"text\"] == page_dict['pageTitle']:\n",
    "            print(\"\\tCorrect\")\n",
    "        else:\n",
    "            print(f\"\\tIncorrect. Got: {res[0][0]['text']}\")\n",
    "        \n",
    "        lat, lon = get_coordinates(res[0][0][\"text\"])\n",
    "        haversine_dist = haversine_distance(float(page_dict['lat']), float(page_dict['lon']), lat, lon)\n",
    "        print(\"\\t\"+str(haversine_dist))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T09:04:32.106186Z",
     "start_time": "2024-03-05T09:04:21.083934Z"
    }
   },
   "id": "2920dc0cd1692de2",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(-38.2333, -72.3333)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates(\"Victoria, Chile\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T09:06:02.558805Z",
     "start_time": "2024-03-05T09:06:01.937005Z"
    }
   },
   "id": "2fe394997849e38b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26bfa9a06f95b357"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
